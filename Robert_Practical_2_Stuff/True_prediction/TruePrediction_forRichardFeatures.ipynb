{
 "metadata": {
  "name": "",
  "signature": "sha256:b3e57812980c8efbbb8bf0dd5a5b7dced267b432c0b38d6caf350411edd17772"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from collections import Counter\n",
      "try:\n",
      "    import xml.etree.cElementTree as ET\n",
      "except ImportError:\n",
      "    import xml.etree.ElementTree as ET\n",
      "import numpy as np\n",
      "from scipy import sparse\n",
      "\n",
      "import util\n",
      "\n",
      "from sklearn import linear_model\n",
      "from sklearn import ensemble\n",
      "\n",
      "\n",
      "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      ffs are a list of feature-functions.\n",
      "      direc is a directory containing xml files (expected to be train or test).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "\n",
      "    returns: \n",
      "      a sparse design matrix, a dict mapping features to column-numbers,\n",
      "      a vector of target classes, and a list of system-call-history ids in order \n",
      "      of their rows in the design matrix.\n",
      "      \n",
      "      Note: the vector of target classes returned will contain the true indices of the\n",
      "      target classes on the training data, but will contain only -1's on the test\n",
      "      data\n",
      "    \"\"\"\n",
      "    fds = [] # list of feature dicts\n",
      "    classes = []\n",
      "    ids = [] \n",
      "    for datafile in os.listdir(direc):\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        ids.append(id_str)\n",
      "        # add target class if this is training data\n",
      "        try:\n",
      "            classes.append(util.malware_classes.index(clazz))\n",
      "        except ValueError:\n",
      "            # we should only fail to find the label in our list of malware classes\n",
      "            # if this is test data, which always has an \"X\" label\n",
      "            assert clazz == \"X\"\n",
      "            classes.append(-1)\n",
      "        rowfd = {}\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        [rowfd.update(ff(tree)) for ff in ffs]\n",
      "        fds.append(rowfd)\n",
      "        \n",
      "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
      "    return X, feat_dict, np.array(classes), ids\n",
      "\n",
      "\n",
      "def make_design_mat(fds, global_feat_dict=None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      fds is a list of feature dicts (one for each row).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "       \n",
      "    returns: \n",
      "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
      "        the union of features defined in any of the fds \n",
      "    \"\"\"\n",
      "    if global_feat_dict is None:\n",
      "        all_feats = set()\n",
      "        [all_feats.update(fd.keys()) for fd in fds]\n",
      "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
      "    else:\n",
      "        feat_dict = global_feat_dict\n",
      "        \n",
      "    cols = []\n",
      "    rows = []\n",
      "    data = []        \n",
      "    for i in xrange(len(fds)):\n",
      "        temp_cols = []\n",
      "        temp_data = []\n",
      "        for feat,val in fds[i].iteritems():\n",
      "            try:\n",
      "                # update temp_cols iff update temp_data\n",
      "                temp_cols.append(feat_dict[feat])\n",
      "                temp_data.append(val)\n",
      "            except KeyError as ex:\n",
      "                if global_feat_dict is not None:\n",
      "                    pass  # new feature in test data; nbd\n",
      "                else:\n",
      "                    raise ex\n",
      "\n",
      "        # all fd's features in the same row\n",
      "        k = len(temp_cols)\n",
      "        cols.extend(temp_cols)\n",
      "        data.extend(temp_data)\n",
      "        rows.extend([i]*k)\n",
      "\n",
      "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
      "   \n",
      "\n",
      "    X = sparse.csr_matrix((np.array(data),\n",
      "                   (np.array(rows), np.array(cols))),\n",
      "                   shape=(len(fds), len(feat_dict)))\n",
      "    return X, feat_dict\n",
      "    \n",
      "\n",
      "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
      "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
      "# feature-names to numeric values.\n",
      "## TODO: modify these functions, and/or add new ones.\n",
      "#binary on first and last system call in all_section\n",
      "def first_last_system_call_feats(tree):\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    first = True # is this the first system call\n",
      "    last_call = None # keep track of last call we've seen\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            if first:\n",
      "                c[\"first_call-\"+el.tag] = 1\n",
      "                first = False\n",
      "            last_call = el.tag  # update last call seen\n",
      "            \n",
      "    # finally, mark last call seen\n",
      "    c[\"last_call-\"+last_call] = 1\n",
      "    return c\n",
      "\n",
      "#number of total system calls in all_section\n",
      "def system_call_count_feats(tree):\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            c['num_system_calls'] += 1\n",
      "    return c\n",
      "\n",
      "#binary on whether each type of call is in all_section\n",
      "def indic_system_calls_by_type(tree):\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            c[\"a_call_type's_indic-\"+el.tag] = 1\n",
      "    return c\n",
      "\n",
      "#number of total system calls in entire file\n",
      "def system_call_count_all(tree):\n",
      "    c = Counter()\n",
      "    for el in tree.iter():\n",
      "        c['num_system_calls_all'] += 1\n",
      "    return c\n",
      "\n",
      "#count number of each system call in all_section\n",
      "def count_system_calls_by_type(tree):\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            c[\"a_call_type's_counter-\"+el.tag] += 1\n",
      "    return c\n",
      "\n",
      "#count max number of consecutive create_mutex calls\n",
      "def max_consec_create_mutex(tree):\n",
      "    c = Counter()\n",
      "    best = 0\n",
      "    current = 0\n",
      "    for el in tree.iter():\n",
      "        if el.tag == \"create_mutex\":\n",
      "            current = current + 1\n",
      "            if current > best:\n",
      "                best = current\n",
      "        else:\n",
      "            current = 0\n",
      "    c['create_mutex_max_block_num'] = best\n",
      "    return c\n",
      "\n",
      "#count number of blocks of 5 consecutive create_mutex calls\n",
      "def five_consec_create_mutex(tree):\n",
      "    c = Counter()\n",
      "    total = 0\n",
      "    current = 0\n",
      "    for el in tree.iter():\n",
      "        if el.tag == \"create_mutex\":\n",
      "            current = current + 1\n",
      "            if current == 5:\n",
      "                total = total + 1\n",
      "                current = 0\n",
      "        else:\n",
      "            current = 0\n",
      "    c['five_mutex_block_num'] = total\n",
      "    return c\n",
      "\n",
      "#binary based on whether one of Kevin's original 3 bad Swizzor sites is found under get_host_by_name\n",
      "def get_host_by_name_bad_ads_indic(tree):\n",
      "    c = Counter()\n",
      "    badadlist = ['ads.netbios-local.com', 'upd.host255-255-255-0.com', 'ads.range159-195.com']\n",
      "    badads = 0\n",
      "    for el in tree.iter():\n",
      "        if el.tag == \"get_host_by_name\":\n",
      "            for website in badadlist:\n",
      "                for key in el.attrib:\n",
      "                    if key == website:\n",
      "                        badads = 1\n",
      "    c['get_host_by_name_bad_ads'] = badads\n",
      "    return c\n",
      "\n",
      "#proportions of each type of system call\n",
      "def proportion_system_calls_by_type(tree):\n",
      "    c = Counter()\n",
      "    d = {}\n",
      "    totalcalls = 0.0\n",
      "    for el in tree.iter():\n",
      "        totalcalls = totalcalls + 1\n",
      "        c[\"a_call_type's_prop_overall-\"+el.tag] += 1\n",
      "    for key in c:\n",
      "        d[key] = c[key] / totalcalls\n",
      "    return d\n",
      "\n",
      "#see how closely correspond to VB 28kb specific counts in all_section\n",
      "def counts_compared_to_small_VB(tree):\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            c[el.tag] += 1\n",
      "    closeness = 0\n",
      "    VBdict = {'load_image':1,'load_dll':26, 'check_for_debugger':1, 'get_system_directory':8,\n",
      "              'open_key':14,'query_value':7,'create_mutex':6, 'set_windows_hook':3,\n",
      "              'create_window':4,'show_window':1,'get_username':1,'find_file':5,\n",
      "              'read_value':2,'get_file_attributes':2,'get_windows_directory':1,\n",
      "              'destroy_window':3,'enum_window':4,'sleep':1,'kill_process':1}\n",
      "    for atag in VBdict:\n",
      "        for key in c:\n",
      "            if atag==key:\n",
      "                if c[key] == VBdict[atag]:\n",
      "                    closeness += 1\n",
      "    d = {\"VB_small_closeness\":closeness}\n",
      "    return d\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "\n",
      "## The following function does the feature extraction, learning, and prediction\n",
      "def main():\n",
      "    train_dir = \"train\"\n",
      "    test_dir = \"test\"\n",
      "    outputfile = \"mypredictions.csv\"  # feel free to change this or take it as an argument\n",
      "    \n",
      "    # TODO put the names of the feature functions you've defined above in this list\n",
      "    ffs = [first_last_system_call_feats,\n",
      "           system_call_count_feats,\n",
      "           indic_system_calls_by_type,\n",
      "           system_call_count_all,\n",
      "           count_system_calls_by_type,\n",
      "           max_consec_create_mutex,\n",
      "           five_consec_create_mutex,\n",
      "           get_host_by_name_bad_ads_indic,\n",
      "           proportion_system_calls_by_type,\n",
      "           counts_compared_to_small_VB]\n",
      "    \n",
      "    # extract features\n",
      "    #print \"extracting training features...\"\n",
      "    #X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
      "    #print \"done extracting training features\"\n",
      "    #print\n",
      "    \n",
      "    # TODO train here, and learn your classification parameters\n",
      "    #print \"learning...\"\n",
      "    #clf = ensemble.RandomForestClassifier(n_estimators=25, max_features=\"sqrt\") \n",
      "    #clf.fit(X_train.toarray(),t_train) #\n",
      "    #print \"done learning\"\n",
      "    #print\n",
      "    \n",
      "    # get rid of training data and load test data\n",
      "    #del X_train\n",
      "    #del t_train\n",
      "    #del train_ids\n",
      "    #print \"extracting test features...\"\n",
      "    #X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
      "    #print \"done extracting test features\"\n",
      "    #print\n",
      "    \n",
      "    # TODO make predictions on text data and write them out\n",
      "    #print \"making predictions...\"\n",
      "    #preds = clf.predict(X_test.toarray())\n",
      "    #print \"done making predictions\"\n",
      "    #print\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    #LOAD EVERYTHING IN FROM CSV\n",
      "    myfile = csv.reader(open(\"combofeatures2.csv\", 'rU'), dialect='excel')\n",
      "    \n",
      "    #Part 1: The feature names. NEED TO ADD NULL filler to A1 for this to work\n",
      "    #next(myfile, None)\n",
      "    \n",
      "    feature_names = []\n",
      "    test_features = np.array([])\n",
      "    train_features = np.array([])\n",
      "    test_ids = []\n",
      "    t_ignore = []\n",
      "    train_target = []\n",
      "    newfeaturerow = np.array([])\n",
      "    newfeaturerow2 = np.array([])\n",
      "    \n",
      "    counter = 0\n",
      "    \n",
      "    for row in myfile:\n",
      "        \n",
      "        if counter < 1:\n",
      "            for index in range(3,1013):\n",
      "                feature_names.append(row[index])\n",
      "        \n",
      "        #Part 2: pull out all the data for training!\n",
      "        else:\n",
      "            train_target.append(int(row[2]))\n",
      "            \n",
      "            newfeaturerow2 = np.array([float(x) for x in row[3:1013]])\n",
      "            \n",
      "            \n",
      "            if counter == 1:\n",
      "                train_features = newfeaturerow2\n",
      "\n",
      "            #stack with the rest else\n",
      "            else:\n",
      "                \n",
      "                train_features = np.vstack((train_features,newfeaturerow2))\n",
      "\n",
      "        counter = counter + 1\n",
      "        \n",
      "    #myfile.close() do I need do close? it's gviing error for now\n",
      "    \n",
      "    \n",
      "    print \"learning...\"\n",
      "    \n",
      "    \n",
      "    clf = ensemble.RandomForestClassifier(n_estimators=5, max_features=\"sqrt\") \n",
      "    clf.fit(train_features,np.asarray(train_target)) #\n",
      "    \n",
      "    \n",
      "    print \"done learning\"\n",
      "    \n",
      "    #test time!!!\n",
      "    myfile2 = csv.reader(open(\"INSERTNAMEHERE\", 'rU'), dialect='excel')\n",
      "    \n",
      "    for row in myfile2:\n",
      "        \n",
      "        if counter < 1:\n",
      "            for index in range(3,1013):\n",
      "                feature_names.append(row[index])\n",
      "        \n",
      "        #Part 2: pull out all the data for training!\n",
      "        else:\n",
      "            test_ids.append(row[1])\n",
      "            #t_ignore.append(int(row[2]))\n",
      "            \n",
      "            newfeaturerow = np.array([float(x) for x in row[3:1013]])\n",
      "            \n",
      "            if counter == 1:\n",
      "                test_features = newfeaturerow\n",
      "            #stack with the rest else\n",
      "            else:\n",
      "                test_features = np.vstack((test_features,newfeaturerow))\n",
      "        \n",
      "        counter = counter + 1\n",
      "    \n",
      "            \n",
      "            \n",
      "                \n",
      "    \n",
      "    #print\n",
      "    \n",
      "    # get rid of training data and load test data\n",
      "    #del X_train\n",
      "    #del t_train\n",
      "    #del train_ids\n",
      "    #print \"extracting test features...\"\n",
      "    #X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
      "    #X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir)\n",
      "    #print t_ignore\n",
      "    \n",
      "    #print \"done extracting test features\"\n",
      "    #print\n",
      "    \n",
      "    # TODO make predictions on text data and write them out\n",
      "    print \"making predictions...\"\n",
      "    #preds = np.argmax(X_test.dot(learned_W),axis=1) #NEED TO EDIT TO USE SCIKIT\n",
      "    \n",
      "    preds = clf.predict(test_features)\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    print \"writing predictions...\"\n",
      "    util.write_predictions(preds, test_ids, outputfile)\n",
      "    print \"done!\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "learning...\n",
        "done learning"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'INSERTNAMEHERE'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-515f13f8275e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-4-515f13f8275e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m#test time!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mmyfile2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INSERTNAMEHERE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'excel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmyfile2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'INSERTNAMEHERE'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}